{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 2 DataScrapping \n",
    "Students:\n",
    "    Edgardo Ortiz\n",
    "    Nicolas Henriquez\n",
    "    Carlo \n",
    "    Benjamin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Importa tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1 Hockey Team Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hockey_url = 'https://www.scrapethissite.com/pages/forms/?page_num=1'\n",
    "\n",
    "# Web scraping\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(hockey_url)\n",
    "data = []\n",
    "\n",
    "while True:\n",
    "    table = driver.find_element(By.TAG_NAME, \"table\")\n",
    "    table_html = table.get_attribute(\"outerHTML\")\n",
    "    soup = BeautifulSoup(table_html, \"html.parser\")\n",
    "\n",
    "    rows = soup.find_all(\"tr\")\n",
    "\n",
    "    for row in rows:\n",
    "        cells = row.find_all(\"td\")  # Change \"td\" to \"th\" if you also want table headers\n",
    "        row_data = [cell.get_text() for cell in cells]\n",
    "        data.append(row_data)\n",
    "    \n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[@aria-label='Next']\")\n",
    "    except NoSuchElementException:\n",
    "        break\n",
    "\n",
    "    next_button.click()\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 Hockey Data Cleanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Create a pandas DataFrame with the scraped data\n",
    "hockey_df = pd.DataFrame(data)\n",
    "\n",
    "# Drop the first row (index 0) of the DataFrame\n",
    "hockey_df = hockey_df.iloc[1:]\n",
    "\n",
    "# Set the headers\n",
    "new_headers = [\"Team Name\", \"Year\", \"Wins\", \"Losses\", \"OT Losses\", \"Win %\", \"Goals For (GF)\", \"Goals Against (GA)\", \"plusMinus\"]\n",
    "hockey_df.columns = new_headers\n",
    "\n",
    "# Replace '\\n' with an empty string '' in all cells of the DataFrame\n",
    "hockey_df = hockey_df.applymap(lambda x: x.replace('\\n', '') if isinstance(x, str) else x)\n",
    "\n",
    "numeric_columns = [\"Year\", \"Wins\", \"Losses\", \"OT Losses\", \"Win %\", \"Goals For (GF)\", \"Goals Against (GA)\", \"plusMinus\"]\n",
    "\n",
    "# Convert numeric columns to numeric data types\n",
    "hockey_df[numeric_columns] = hockey_df[numeric_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Drop rows that are completely empty\n",
    "hockey_df = hockey_df.dropna(how='all')\n",
    "\n",
    "# Replace NaN with 0 in the \"OT Losses\" column\n",
    "hockey_df[\"OT Losses\"].fillna(0, inplace=True)\n",
    "\n",
    "# Remove leading and trailing white spaces from the \"Team Name\" column\n",
    "hockey_df[\"Team Name\"] = hockey_df[\"Team Name\"].str.strip()\n",
    "\n",
    "# Save the cleaned DataFrame to an Excel file\n",
    "hockey_df.to_excel(\"hockey_cleaned.xlsx\", index=False)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "hockey_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.1 Oscar Winners Scrapping\n",
    "\n",
    "Meterse a wikipedia, obtener desde 1980 y obtener director y cuanto costo y recaudo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()\n",
    "oscar_url = 'https://www.scrapethissite.com/pages/ajax-javascript/'\n",
    "driver.get(oscar_url)\n",
    "\n",
    "year_buttons = driver.find_elements(By.CLASS_NAME, \"year-link\")\n",
    "\n",
    "data = []\n",
    "\n",
    "for button in year_buttons:\n",
    "    time.sleep(5)\n",
    "    year_id = button.get_attribute(\"id\")\n",
    "    year_text = button.text\n",
    "    print(year_text)\n",
    "\n",
    "    # Hacer clic en el botón para obtener más datos si es necesario\n",
    "    button.click()\n",
    "\n",
    "    table = driver.find_element(By.TAG_NAME, \"table\")\n",
    "    table_html = table.get_attribute(\"outerHTML\")\n",
    "    soup = BeautifulSoup(table_html, \"html.parser\")\n",
    "\n",
    "    rows = soup.find_all(\"tr\")\n",
    "\n",
    "    for row in rows:\n",
    "        cells = row.find_all(\"td\")  # Change \"td\" to \"th\" if you also want table headers\n",
    "        row_data = [year_text]\n",
    "\n",
    "        for cell in cells:\n",
    "\n",
    "            if \"film-best-picture\" in cell.get(\"class\", []):\n",
    "                # Busca el ícono \"glyphicon-flag\" dentro de la celda\n",
    "                flag_icon = cell.find(\"i\", class_=\"glyphicon glyphicon-flag\")\n",
    "                # Verifica si se encontró el ícono y agrega \"True\" o \"False\" en consecuencia\n",
    "                best_picture = \"True\" if flag_icon else \"False\"\n",
    "                row_data.append(best_picture)\n",
    "            else:\n",
    "                row_data.append(cell.get_text())\n",
    "\n",
    "        data.append(row_data)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2 Awards Cleanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de pandas con los datos\n",
    "column_names = [\"Year\", \"Title\", \"Nominations\", \"Awards\", \"Best Picture\"]\n",
    "df_awards = pd.DataFrame(data, columns=column_names)\n",
    "# Convertir \"Nominations\" y \"Awards\" a números\n",
    "df_awards[\"Nominations\"] = pd.to_numeric(df_awards[\"Nominations\"], errors=\"coerce\")\n",
    "df_awards[\"Awards\"] = pd.to_numeric(df_awards[\"Awards\"], errors=\"coerce\")\n",
    "# Convertir \"Año\" a un número\n",
    "df_awards[\"Year\"] = pd.to_numeric(df_awards[\"Year\"], errors=\"coerce\")\n",
    "df_awards = df_awards.dropna(thresh=3)\n",
    "df_awards.to_excel(\"awards_cleaned.xlsx\", index=False)\n",
    "df_awards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.1 Turtles Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()\n",
    "\n",
    "turtle_url = 'https://www.scrapethissite.com/pag1`es/frames/'\n",
    "driver.get(turtle_url)\n",
    "\n",
    "iframe = driver.find_element(By.ID, 'iframe')\n",
    "driver.switch_to.frame(iframe)\n",
    "\n",
    "learn_more_buttons = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_all_elements_located((By.CLASS_NAME, 'btn.btn-default.btn-xs'))\n",
    ")\n",
    "\n",
    "href_values = []\n",
    "\n",
    "for button in learn_more_buttons:\n",
    "    href_value = button.get_attribute(\"href\")\n",
    "    if href_value:\n",
    "        href_values.append(href_value)\n",
    "\n",
    "lead_list = []\n",
    "image_src_list = []\n",
    "\n",
    "for href in href_values:\n",
    "    driver.get(href)\n",
    "    time.sleep(2)\n",
    "\n",
    "    image_element = driver.find_element(By.CLASS_NAME, 'turtle-image.center-block')\n",
    "    image_src = image_element.get_attribute(\"src\")\n",
    "    image_src_list.append(image_src)\n",
    "\n",
    "    lead_element = driver.find_element(By.CLASS_NAME, 'lead')\n",
    "    lead_text = lead_element.text\n",
    "    lead_list.append(lead_text)\n",
    "\n",
    "driver.switch_to.default_content()\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.2 Turtles transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex\n",
    "regex = r'The (.*?) family of turtles — more commonly known as \"(.*?)\" — were first discovered in (\\d{4}) by ([^.]+)\\.'\n",
    "\n",
    "# List to store information\n",
    "turtle_info = []\n",
    "\n",
    "# Iterate through each lead element\n",
    "for lead_text in lead_list:\n",
    "    # Apply the regex to extract information\n",
    "    result = re.search(regex, lead_text)\n",
    "    \n",
    "    # Check if the pattern was found\n",
    "    if result:\n",
    "        name = result.group(1)\n",
    "        common_name = result.group(2)\n",
    "        discovery_year = result.group(3)\n",
    "        discovered_by = result.group(4)\n",
    "    else:\n",
    "        name = \"Information not found\"\n",
    "        common_name = \"Information not found\"\n",
    "        discovery_year = \"Information not found\"\n",
    "        discovered_by = \"Information not found\"\n",
    "    \n",
    "    # Add the information to the list\n",
    "    turtle_info.append({\n",
    "        'name': name,\n",
    "        'common_name': common_name,\n",
    "        'discovery_year': discovery_year,\n",
    "        'discovered_by': discovered_by\n",
    "    })\n",
    "\n",
    "turtle_df = pd.DataFrame(turtle_info)\n",
    "\n",
    "turtle_df['image_url'] = image_src_list\n",
    "\n",
    "# Show the DataFrame with the new column\n",
    "turtle_df.to_excel(\"turtle_cleaned.xlsx\", index=False)\n",
    "turtle_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Secret Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Secret Task Awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "options = FirefoxOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "# Inicializa el controlador de Firefox\n",
    "driver = webdriver.Firefox(options=options)\n",
    "\n",
    "awards_secret_url = 'https://en.wikipedia.org/wiki/List_of_Academy_Award-winning_films'\n",
    "driver.get(awards_secret_url)\n",
    "time.sleep(5)\n",
    "\n",
    "data = []\n",
    "\n",
    "table = driver.find_element(By.TAG_NAME, \"table\")\n",
    "table_html = table.get_attribute(\"outerHTML\")\n",
    "soup = BeautifulSoup(table_html, \"html.parser\")\n",
    "\n",
    "# Encuentra todas las filas de la tabla (supongo que están en una etiqueta 'tr')\n",
    "rows = soup.find_all('tr')\n",
    "\n",
    "for row in rows:\n",
    "    if 'style' in row.attrs and 'background' in row['style']:\n",
    "        has_background = True\n",
    "    else:\n",
    "        has_background = False\n",
    "\n",
    "    columns = row.find_all('td')\n",
    "    if len(columns) >= 4:\n",
    "        # Inicializa variables con valores predeterminados\n",
    "        nombre = ''\n",
    "        href = ''\n",
    "        año = ''\n",
    "        premios = '0'\n",
    "        nominaciones = '0'\n",
    "\n",
    "        # Intenta extraer los datos si están disponibles\n",
    "        if columns[0].find('i'):\n",
    "            nombre = columns[0].find('i').text.strip()\n",
    "            if columns[0].find('a'):\n",
    "                href = columns[0].find('a')['href']\n",
    "        if columns[1].text.strip():\n",
    "            año = columns[1].text.strip()\n",
    "        if columns[2].text.strip():\n",
    "            premios = columns[2].text.strip()\n",
    "        if columns[3].text.strip():\n",
    "            nominaciones = columns[3].text.strip()\n",
    "\n",
    "        # Imprime si la fila tiene fondo o no\n",
    "        if has_background:\n",
    "            print(\"Esta fila tiene fondo:\")\n",
    "        else:\n",
    "            print(\"Esta fila no tiene fondo:\")\n",
    "\n",
    "        # Imprime los datos que has extraído\n",
    "        print(f'Nombre: {nombre}')\n",
    "        print(f'Año: {año}')\n",
    "        print(f'Premios: {premios}')\n",
    "        print(f'Nominaciones: {nominaciones}')\n",
    "        if href:\n",
    "            link = f'https://en.wikipedia.org{href}'\n",
    "            print(f'Href: {link}')\n",
    "        else:\n",
    "            link = ''\n",
    "            print('No se encontró un enlace.')\n",
    "\n",
    "        print()\n",
    "        \n",
    "\n",
    "        # Agrega los datos a la lista data\n",
    "        data.append({\n",
    "            'Title': nombre,\n",
    "            'Year': año,\n",
    "            'Awards': premios,\n",
    "            'Nominations': nominaciones,\n",
    "            'IsBestpicture': 1 if has_background else 0,\n",
    "            'Link': link\n",
    "        })\n",
    "\n",
    "# Cierra el controlador de Firefox\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_secret_df = pd.DataFrame(data)\n",
    "awards_secret_df = awards_secret_df[awards_secret_df['Year'] >= '1980']\n",
    "awards_secret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scrape_website(link):\n",
    "    if not link:\n",
    "        print(\"El enlace está vacío.\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    driver.get(link)\n",
    "    time.sleep(3) \n",
    "\n",
    "    budget = None\n",
    "    box_office = None\n",
    "\n",
    "    try:\n",
    "        budget_element = driver.find_element(By.XPATH, \"//th[contains(text(),'Budget')]/following-sibling::td[@class='infobox-data']\")\n",
    "        budget = budget_element.text.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"No se encontró el elemento 'Budget'. Error: {e}\")\n",
    "        print(link)\n",
    "\n",
    "    try:\n",
    "        # Intenta encontrar el elemento de la taquilla\n",
    "        box_office_element = driver.find_element(By.XPATH, \"//th[contains(text(),'Box office')]/following-sibling::td[@class='infobox-data']\")\n",
    "        box_office = box_office_element.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"No se encontró el elemento 'Box office'. Error: {e}\")\n",
    "        print(link)\n",
    "\n",
    "    driver.quit()  \n",
    "\n",
    "    budget = parse_budget(budget)\n",
    "    box_office = parse_box_office(box_office)\n",
    "\n",
    "    return budget, box_office\n",
    "\n",
    "def parse_budget(budget):\n",
    "    if not budget:\n",
    "        return 0\n",
    "    try:\n",
    "        \n",
    "        budget = re.sub(r'\\[[0-9]+\\]', '', budget) \n",
    "        matches = re.findall(r'([\\d.]+)', budget) \n",
    "        if matches:\n",
    "           \n",
    "            values = [float(match) for match in matches]\n",
    "            return min(values)\n",
    "        else:\n",
    "            return 0\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "def parse_box_office(box_office):\n",
    "    if not box_office:\n",
    "        return 0\n",
    "    try:\n",
    "        box_office = re.sub(r'\\[[0-9]+\\]', '', box_office)  \n",
    "        matches = re.findall(r'([\\d.]+)', box_office)  \n",
    "        if matches:\n",
    "            # Convierte los valores a flotantes y toma el valor más bajo\n",
    "            values = [float(match) for match in matches]\n",
    "            return min(values)\n",
    "        else:\n",
    "            return 0\n",
    "    except ValueError:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenues = []\n",
    "box_offices = []\n",
    "# Itera a través de las filas del DataFrame\n",
    "for index, row in tqdm(awards_secret_df.iterrows(), total=len(awards_secret_df)):\n",
    "    link = row['Link']\n",
    "    revenue, box_office = scrape_website(link)\n",
    "    revenues.append(revenue)\n",
    "    box_offices.append(box_office)\n",
    "\n",
    "# Crea dos Series a partir de las listas de valores\n",
    "revenue_series = pd.Series(revenues, name='Revenue')\n",
    "box_office_series = pd.Series(box_offices, name='Box Office')\n",
    "\n",
    "# Agrega las Series al DataFrame original\n",
    "awards_secret_df['Revenue'] = revenue_series\n",
    "awards_secret_df['Box Office'] = box_office_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_valor(valor):\n",
    "    valor_limpio = re.sub(r'[^0-9]', '', valor)\n",
    "    return valor_limpio\n",
    "awards_secret_df[\"Awards\"] = awards_secret_df[\"Awards\"].apply(limpiar_valor)\n",
    "awards_secret_df[\"Nominations\"] = awards_secret_df[\"Nominations\"].apply(limpiar_valor)\n",
    "\n",
    "\n",
    "\n",
    "# Save the cleaned DataFrame to an Excel file\n",
    "awards_secret_df.to_excel(\"awards_secret.xlsx\", index=False)\n",
    "awards_secret_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Turtle Secret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
